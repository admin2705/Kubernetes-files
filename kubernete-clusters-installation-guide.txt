kubernetes note:

connecting ec2 server using SSH client.
1. open terminal   > cd Downloads/    <--- open folder where your key is present.
2.chmod 400 key.pem
3. paste ssh command here.
4.

create one ec2 instance:
download KIND  cluster and  kubectl: using shell script:

------------------------------------------------------------------------------------------------------------------------------------------------------------
1. Installing KIND Cluster and kubectl
cmd =>vim install_kind.sh

#!/bin/bash

[ $(uname -m) = x86_64 ] && curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.20.0/kind-linux-amd64
chmod +x ./kind
sudo cp ./kind /usr/local/bin/kind

VERSION="v1.30.0"
URL="https://dl.k8s.io/release/${VERSION}/bin/linux/amd64/kubectl"
INSTALL_DIR="/usr/local/bin"

curl -LO "$URL"
chmod +x kubectl
sudo mv kubectl $INSTALL_DIR/
kubectl version --client

rm -f kubectl
rm -rf kind

echo "kind & kubectl installation complete."
------------------------------------------------------------------------------------------------------------------------------------------------------------

vim install_kind.sh           ----> save & quit.
chmod 777 install_kind.sh
./install_kind.sh
sudo apt get update           ----> wil download all packages.
sudo apt-get install docker.io
docker ps                     ----> will check docker installed or not.[permission denied]
sudo usermod -aG docker $USER && newgrp docker
docker ps
docker   --version
kubectl version
kind     --version
------------------------------------------------------------------------------------------------------------------------------------------------------------
I want to create kubernetes Cluster:configuration of Cluster
in which i want 3 node and 1 master node:

cmd ==> vim config.yml

kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4       ----> we can find it on kind offical website(configuration)

nodes:
- role: control-plane
  image: kindest/node:v1.31.2           ----> control plane will create by using Docker image ["kindest" is one of the Docker img "/node" means create note with version]
-role: worker                           ----> creating worker nodes.
  image: kindest/node:v1.31.2
-role: worker
  image: kindest/node:v1.31.2
-role: worker
  image: kindest/node:v1.31.2
  extraportmapping:                     ----> Binding ports[here we are binding container ports with host port]
  - containerport: 80 
    hostPort: 80
    protocol: TCP    
  - containerport: 443 
    hostPort: 443
    protocol: TCP    

------------------------------------------------------------------------------------------------------------------------------------------------------------
commands:

kind create cluster --name=tws-cluster --config=config.yml

kubectl cluster-info --context kind-tws-cluster      ----> will show your cluster information

kubectl get nodes                                    ----> will show available nodes details

docker enable
####################################################################################################################################################################
before insatlling minikube install docker
Now we will se creating nodes using Minikube:

curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64
chmod +x minikube
sudo mv minikube /usr/local/bin/  
minikube version
curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
chmod +x kubectl
sudo mv kubectl /usr/local/bin/
minikube start --driver=docker --vm=true       ----> write vm=true only when you are working on ec2
kubectl get nodes --context kind-tws-cluster
minikube stop
minikube delete

####################################################################################################################################################################
------------------------------------------------------------------------------------------------------------------------------------------------------------

installing Kubeadm:

//Create two EC2 servers for demo purpose: 1. masternode & worker node.

Step 1: Identify or Create a Security Group

1. Log in to the AWS Management Console:
   Go to the EC2 Dashboard.

2. Locate Security Groups:
   In the left menu under Network & Security, click on Security Groups.

3. Create a New Security Group:
   Click on Create Security Group.
   Provide the following details:
   Name: (e.g., Kubernetes-Cluster-SG)
   Description: A brief description for the security group (mandatory)
   VPC: Select the appropriate VPC for your instances (default is acceptable)

4. Add Rules to the Security Group:
   Allow SSH Traffic (Port 22):
   Type: SSH
   Port Range: 22
   Source: 0.0.0.0/0 (Anywhere) or your specific IP
   
   Allow Kubernetes API Traffic (Port 6443):
   Type: Custom TCP
   Port Range: 6443
   Source: 0.0.0.0/0 (Anywhere) or specific IP ranges

5. Save the Rules:
Click on Create Security Group to save the settings.


//Step 2: Select the Security Group While Creating Instances
   When launching EC2 instances:
   Under Configure Security Group, select the existing security group (Kubernetes-Cluster-SG)

------------------------------------------------------------------------------------------------------------------------------------------------------------

Execute on Both "Master" & "Worker" Nodes

1. Disable Swap: Required for Kubernetes to function correctly.
sudo swapoff -a

2. Load Necessary Kernel Modules: Required for Kubernetes networking.

cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF

sudo modprobe overlay
sudo modprobe br_netfilter

3. Set Sysctl Parameters: Helps with networking.

cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward                 = 1
EOF

sudo sysctl --system
lsmod | grep br_netfilter
lsmod | grep overlay

4. Install Containerd:

sudo apt-get update
sudo apt-get install -y ca-certificates curl
sudo install -m 0755 -d /etc/apt/keyrings
sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc
sudo chmod a+r /etc/apt/keyrings/docker.asc

echo "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu $(. /etc/os-release && echo \"$VERSION_CODENAME\") stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null

sudo apt-get update
sudo apt-get install -y containerd.io

containerd config default | sed -e 's/SystemdCgroup = false/SystemdCgroup = true/' -e 's/sandbox_image = "registry.k8s.io\/pause:3.6"/sandbox_image = "registry.k8s.io\/pause:3.9"/' | sudo tee /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl status containerd

5. Install Kubernetes Components:

sudo apt-get update
sudo apt-get install -y apt-transport-https ca-certificates curl gpg

curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.29/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg

echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.29/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list

sudo apt-get update
sudo apt-get install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl

------------------------------------------------------------------------------------------------------------------------------------------------------------
// Execute ONLY on the "Master" Node


1. Initialize the Cluster:
sudo kubeadm init

2. Set Up Local kubeconfig:
>>>here i am creating diectory to save all configuration and changing owner of 
then your current user also work like root on kubeadl.


mkdir -p "$HOME"/.kube
sudo cp -i /etc/kubernetes/admin.conf "$HOME"/.kube/config
sudo chown "$(id -u)":"$(id -g)" "$HOME"/.kube/config

3. Install a Network Plugin (Calico):  >> here i am appling callico network on your clustor.

kubectl apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.26.0/manifests/calico.yaml


4. Generate Join Command: >> will create one token then copy that token and add it on your worker node.
kubeadm token create --print-join-command

------------------------------------------------------------------------------------------------------------------------------------------------------------
// Execute on ALL of your Worker Nodes
1. Perform pre-flight checks:
sudo kubeadm reset pre-flight checks

2.Paste the join command you got from the master node and append --v=5 at the end:

sudo <paste-join-command-here> --v=5

3. kubectl get nodes




   
